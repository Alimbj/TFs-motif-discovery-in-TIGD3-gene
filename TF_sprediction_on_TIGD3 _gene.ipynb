{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebc499-b8a8-41a3-972f-7f58996ed3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_recall_curve, auc, roc_curve, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce745b-32e7-4f65-a8de-a6d5dd54f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of TSV files to read\n",
    "file_names = [\n",
    "    '1ibis_QNZS_PBM13699.tsv', '1ibis_SD_PBM13699.tsv'\n",
    "]\n",
    "\n",
    "# Read each TSV file into a DataFrame and store in a list\n",
    "dataframes = [pd.read_csv(file, sep='\\t') for file in file_names]\n",
    "\n",
    "# Assign each DataFrame to a variable\n",
    "data1, data2 = dataframes\n",
    "\n",
    "# Process QNZS-normalized PBMs\n",
    "for df in [data1]:\n",
    "    df['z_score'] = (df['mean_signal_intensity'] - df['mean_signal_intensity'].mean()) / df['mean_signal_intensity'].std()\n",
    "    df['label'] = (df['z_score'] > 4).astype(int)\n",
    "\n",
    "# Process SD-preprocessed PBMs\n",
    "for df in [data2]:\n",
    "    mean_intensity = df['mean_signal_intensity'].mean()\n",
    "    std_dev_intensity = df['mean_signal_intensity'].std()\n",
    "    threshold = mean_intensity + 4 * std_dev_intensity\n",
    "    df['label'] = (df['mean_signal_intensity'] > threshold).astype(int)\n",
    "\n",
    "# Iterate through the dataframes and print the sum count of label == 1\n",
    "for i, df in enumerate(dataframes, 1):\n",
    "    label_count = df['label'].sum()\n",
    "    print(f\"Sum count of label == 1 in data {i}: {label_count}\")\n",
    "\n",
    "# Concatenate two PBM datasets\n",
    "concatenated_data = pd.concat([data1, data2])\n",
    "\n",
    "# Extract two columns with aimed data and rename column as needed\n",
    "concatenated_data = concatenated_data[['pbm_sequence', 'label']]\n",
    "concatenated_data = concatenated_data.rename(columns={'pbm_sequence': 'sequence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4ae73-5cf2-4509-aec2-a899726cf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the reverse complement of a DNA sequence\n",
    "def reverse_complement(seq):\n",
    "    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
    "    return ''.join(complement.get(base, base) for base in reversed(seq))\n",
    "\n",
    "# Function to shift DNA sequences by a given number of positions\n",
    "def shift_sequence(seq, shift):\n",
    "    return seq[shift:] + seq[:shift]\n",
    "\n",
    "# Function to perform nucleotide substitution at the first and last two positions\n",
    "def mutate_edges(seq):\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    mutated_sequences = []\n",
    "    # Generate all possible mutations for the first and last two positions\n",
    "    for i1 in [0, 1]:\n",
    "        for i2 in [-2, -1]:\n",
    "            for _ in range(4):  # Attempt four mutations\n",
    "                mutated_seq = list(seq)\n",
    "                mutated_seq[i1] = random.choice([b for b in bases if b != seq[i1]])\n",
    "                mutated_seq[i2] = random.choice([b for b in bases if b != seq[i2]])\n",
    "                mutated_sequences.append(''.join(mutated_seq))\n",
    "    return mutated_sequences\n",
    "\n",
    "\n",
    "# Apply augmentation to positive cases\n",
    "# Filter out the sequences where label = 1\n",
    "label_1_sequences = concatenated_data[concatenated_data['label'] == 1]['sequence']\n",
    "\n",
    "augmented_sequences = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Apply augmentations\n",
    "for seq in label_1_sequences:\n",
    "    # Original sequence\n",
    "    augmented_sequences.append(seq)\n",
    "    augmented_labels.append(1)\n",
    "    \n",
    "    # Reverse complement\n",
    "    rev_comp_seq = reverse_complement(seq)\n",
    "    augmented_sequences.append(rev_comp_seq)\n",
    "    augmented_labels.append(1)\n",
    "    \n",
    "    # Shift by 1 and 2 positions (both original and reverse complement)\n",
    "    for shift in [1, 2]:\n",
    "        shifted_seq_fwd = shift_sequence(seq, shift)\n",
    "        augmented_sequences.append(shifted_seq_fwd)\n",
    "        augmented_labels.append(1)\n",
    "        \n",
    "        shifted_seq_rev = shift_sequence(rev_comp_seq, shift)\n",
    "        augmented_sequences.append(shifted_seq_rev)\n",
    "        augmented_labels.append(1)\n",
    "        \n",
    "    # Mutate first and last two nucleotides in the original and reverse complement sequences\n",
    "    mutated_seqs_original = mutate_edges(seq)\n",
    "    mutated_seqs_rev_comp = mutate_edges(rev_comp_seq)\n",
    "    \n",
    "    for mutated_seq in mutated_seqs_original:\n",
    "        augmented_sequences.append(mutated_seq)\n",
    "        augmented_labels.append(1)\n",
    "        \n",
    "    for mutated_seq in mutated_seqs_rev_comp:\n",
    "        augmented_sequences.append(mutated_seq)\n",
    "        augmented_labels.append(1)\n",
    "\n",
    "# Create a DataFrame for the augmented sequences\n",
    "augmented_data = pd.DataFrame({\n",
    "    'sequence': augmented_sequences,\n",
    "    'label': augmented_labels\n",
    "})\n",
    "\n",
    "# Combine the augmented data with the original data_combined dataset\n",
    "data_combined_augmented = pd.concat([concatenated_data, augmented_data], ignore_index=True)\n",
    "\n",
    "print(f\"Original class distribution:\\n{concatenated_data['label'].value_counts()}\")\n",
    "print(f\"Augmented class distribution:\\n{data_combined_augmented['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba07e3f-aa77-4a42-b575-084e02376374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets from HTS and SMS methods\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Define file names and corresponding labels\n",
    "fastq_files = [\n",
    "    ('1_ibis_HTS_TIGD3_R0_C4_lf5ACGACGCTCTTCCGATCTGC_rf3CACTTCAGATCGGAAGAGCA.fastq', 1),\n",
    "    # ('1_ibis_hts_LEF1_R0_C3_lf5ACGACGCTCTTCCGATCTAT_rf3AGCCTCAGATCGGAAGAGCA.fastq', 1), ('1_ibis_sms_TIGD3_UT380-225.fastq', 1)  # this two datasets were filtered out to more perfect model perfomance\n",
    "]\n",
    "\n",
    "# Define the minimum average quality score threshold\n",
    "min_avg_quality = 37  # Adjust this threshold as needed\n",
    "\n",
    "# Function to calculate the average quality score from a quality string\n",
    "def calculate_avg_quality(quality_string):\n",
    "    # Convert ASCII characters to Phred quality scores by subtracting 33\n",
    "    return np.mean([ord(char) - 33 for char in quality_string])\n",
    "\n",
    "# Loop through the files and extract sequences\n",
    "for file_name, label in fastq_files:\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for i in range(0, len(lines), 4):  # FASTQ format: 4 lines per sequence entry\n",
    "        sequence = lines[i + 1].strip()  # The sequence is on the 2nd line of every 4-line entry\n",
    "        quality = lines[i + 3].strip()   # The quality string is on the 4th line of every 4-line entry\n",
    "        \n",
    "        # Extract the last 40 base pairs of the sequence and their quality scores\n",
    "        last_40_bp = sequence[-40:]\n",
    "        last_40_quality = quality[-40:]\n",
    "\n",
    "        # Calculate the average quality of the last 40 base pairs\n",
    "        avg_quality = calculate_avg_quality(last_40_quality)\n",
    "        \n",
    "        # Check if the average quality meets the threshold\n",
    "        if avg_quality >= min_avg_quality:\n",
    "            sequences.append(last_40_bp)\n",
    "            labels.append(label)  # Add the label for each sequence\n",
    "\n",
    "# Create a DataFrame from the extracted sequences and labels\n",
    "df_fastq = pd.DataFrame({\n",
    "    'sequence': sequences,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Check if all sequences are of length 40\n",
    "correct_length = all(len(seq) == 40 for seq in df_fastq['sequence'])\n",
    "if correct_length:\n",
    "    print(\"All sequences are of length 40.\")\n",
    "else:\n",
    "    print(\"There are sequences that are not of length 40.\")\n",
    "\n",
    "# Check if all sequences contain only A, C, G, T\n",
    "valid_bases = set('ACGT')\n",
    "invalid_sequences = df_fastq['sequence'].apply(lambda seq: not set(seq).issubset(valid_bases))\n",
    "\n",
    "# Report invalid sequences\n",
    "if invalid_sequences.any():\n",
    "    print(f\"Found {invalid_sequences.sum()} sequences with invalid characters:\")\n",
    "    print(df_fastq[invalid_sequences])\n",
    "else:\n",
    "    print(\"All sequences contain only valid characters (A, C, G, T).\")\n",
    "\n",
    "# Concatenate the new DataFrame with df3\n",
    "df3 = pd.concat([data_combined_augmented, df_fastq], ignore_index=True)\n",
    "\n",
    "# Check the concatenated DataFrame\n",
    "print(df3['label'].value_counts())\n",
    "\n",
    "# # Filter out sequences that contain 'N' insteed of nucleotide\n",
    "df3 = df3[~df3['sequence'].str.contains('N')]\n",
    "\n",
    "# # Verify that there are no sequences with 'N'\n",
    "assert df3['sequence'].str.contains('N').sum() == 0, \"There are still sequences containing 'N'!\"\n",
    "df3['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bab50f-f737-49d1-b7e6-13eba6def4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, after fine tuning model with validation dataset, i train model in all dataset as train\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device available: {device}')\n",
    "\n",
    "# Data preparation\n",
    "X_dna = df3['sequence']\n",
    "y = df3['label']\n",
    "\n",
    "# Function to generate random DNA sequence based on the nucleotide distribution\n",
    "def generate_random_dna(length, base_probabilities):\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    return ''.join(random.choices(bases, weights=base_probabilities, k=length))\n",
    "\n",
    "# Function to compute nucleotide distribution\n",
    "def compute_base_probabilities(sequences):\n",
    "    total_len = sum(len(seq) for seq in sequences)\n",
    "    base_counts = {base: sum(seq.count(base) for seq in sequences) for base in 'ACGT'}\n",
    "    return [base_counts[base] / total_len for base in 'ACGT']\n",
    "\n",
    "# Function to pad shorter sequences\n",
    "def pad_sequences_with_random(sequences, max_len, opposite_base_probs):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padding_length = max_len - len(seq)\n",
    "        if padding_length > 0:\n",
    "            random_seq = generate_random_dna(padding_length, opposite_base_probs)\n",
    "            padded_seq = seq + random_seq\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences\n",
    "\n",
    "# Function to get the reverse complement of a DNA sequence\n",
    "def reverse_complement(seq):\n",
    "    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
    "    return ''.join(complement.get(base, base) for base in reversed(seq))\n",
    "\n",
    "# Function to shift DNA sequences\n",
    "def shift_sequence(seq, shift):\n",
    "    return seq[shift:] + seq[:shift]\n",
    "\n",
    "# Function to generate k-mers from a sequence\n",
    "def generate_variable_kmers(sequence, k_sizes):\n",
    "    kmers = []\n",
    "    for k in k_sizes:\n",
    "        kmers.extend([''.join(map(str, sequence[i:i+k])) for i in range(len(sequence) - k + 1)])\n",
    "    return kmers\n",
    "\n",
    "# Function to encode k-mers to integers\n",
    "def integer_encode_variable_kmers(sequence, k_sizes):\n",
    "    kmers = generate_variable_kmers(sequence, k_sizes)\n",
    "    integer_encoded = [kmer_to_int[kmer] for kmer in kmers if kmer in kmer_to_int]\n",
    "    return integer_encoded\n",
    "\n",
    "# Augment sequences where y = 0 for training data\n",
    "augmented_train_sequences = []\n",
    "augmented_train_labels = []\n",
    "\n",
    "shift_value = 3  # can be adjasted\n",
    "\n",
    "for seq, label in zip(X_dna, y):\n",
    "    if label == 0:\n",
    "        # Apply reverse complement\n",
    "        rev_comp_seq = reverse_complement(seq)\n",
    "        augmented_train_sequences.append(rev_comp_seq)\n",
    "        augmented_train_labels.append(0)\n",
    "        \n",
    "        # Apply shift sequence to the original and reverse complement sequences\n",
    "        for i in range(1, shift_value + 1):  # Shift by 1 to shift_value positions\n",
    "            shifted_seq_original = shift_sequence(seq, i)\n",
    "            shifted_seq_rev_comp = shift_sequence(rev_comp_seq, i)\n",
    "            \n",
    "            # Store the shifted sequences and labels\n",
    "            augmented_train_sequences.append(shifted_seq_original)\n",
    "            augmented_train_sequences.append(shifted_seq_rev_comp)\n",
    "            augmented_train_labels.append(0)\n",
    "            augmented_train_labels.append(0)\n",
    "\n",
    "# Combine the original sequences with augmented sequences\n",
    "X_train_augmented = pd.Series(X_dna.tolist() + augmented_train_sequences)\n",
    "y_train_augmented = pd.Series(y.tolist() + augmented_train_labels)\n",
    "\n",
    "# Print the sizes of the original and augmented training datasets\n",
    "print(f'The original train dataset size: {X_dna.shape[0]}')\n",
    "print(f'The augmented train dataset size: {X_train_augmented.shape[0]}')\n",
    "\n",
    "# Print the class distribution in the augmented training dataset\n",
    "print(f'Class distribution in augmented train dataset:\\n{y_train_augmented.value_counts()}')\n",
    "\n",
    "# Calculate nucleotide probabilities from the negative dataset, this sequences will be used in padding\n",
    "negative_sequences = [seq for seq in data_combined_augmented[data_combined_augmented['label'] == 0]['sequence']]  # as negatives can be choosen any class\n",
    "negative_base_probs = compute_base_probabilities(negative_sequences)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = max(len(seq) for seq in X_dna)\n",
    "X_train_padded = pad_sequences_with_random(X_train_augmented, max_len, negative_base_probs)\n",
    "\n",
    "# create k-mers - the DNA seq of len(k)\n",
    "k_sizes = [2, 5]  # k-mer lengths can be optimised as you want\n",
    "kmers = [''.join(p) for k in k_sizes for p in product('ACGT', repeat=k)]\n",
    "kmer_to_int = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "# Encode padded sequences\n",
    "X_train_encoded = [torch.tensor(integer_encode_variable_kmers(seq, k_sizes), dtype=torch.long) for seq in X_train_padded]\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train_augmented.values, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(torch.stack(X_train_encoded), y_train_tensor)\n",
    "\n",
    "# Set hyperparameters\n",
    "num_embeddings = len(kmer_to_int)\n",
    "embedding_dim = 130  \n",
    "num_filters = 120\n",
    "batch_size = 64\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# model architecture\n",
    "class SimplifiedMaskedCNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, num_filters, dropout_rate):\n",
    "        super(SimplifiedMaskedCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, num_filters, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters * 2, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        # self.conv3 = nn.Conv1d(num_filters * 2, num_filters * 4, kernel_size=3, padding=1)\n",
    "        # self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        # self.conv4 = nn.Conv1d(num_filters * 4, num_filters * 8, kernel_size=3, padding=1)\n",
    "        # self.pool4 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv5 = nn.Conv1d(num_filters * 2, num_filters * 4, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(4 * num_filters, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        x = F.relu(self.pool1(self.conv1(x)))\n",
    "        x = F.relu(self.pool2(self.conv2(x)))\n",
    "        # x = F.relu(self.pool3(self.conv3(x)))\n",
    "        # x = F.relu(self.pool4(self.conv4(x)))\n",
    "        x = F.relu(self.pool5(self.conv5(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Focal Loss definition\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.5):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.unsqueeze(1)\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "# Instantiate the model\n",
    "cnn = SimplifiedMaskedCNN(num_embeddings, embedding_dim, num_filters, dropout_rate).to(device)\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.AdamW(cnn.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "\n",
    "# Initialize variables for metrics storage\n",
    "train_losses = []\n",
    "all_train_targets, all_train_scores = [], []\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    cnn.train()\n",
    "    train_loss = 0\n",
    "    train_targets, train_scores = [], []\n",
    "\n",
    "    # Training loop\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(target)\n",
    "\n",
    "        # Collect scores for metrics\n",
    "        train_targets.extend(target.cpu().numpy())\n",
    "        train_scores.extend(torch.sigmoid(outputs).detach().cpu().numpy().flatten())\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    all_train_targets.extend(train_targets)\n",
    "    all_train_scores.extend(train_scores)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_precision, train_recall, _ = precision_recall_curve(train_targets, train_scores)\n",
    "    train_auprc = auc(train_recall, train_precision)\n",
    "    train_fpr, train_tpr, _ = roc_curve(train_targets, train_scores)\n",
    "    train_auroc = auc(train_fpr, train_tpr)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "          f'Train AUPRC: {train_auprc:.4f}, Train AUROC: {train_auroc:.4f}')\n",
    "\n",
    "    # Compute and display the confusion matrix for the current epoch\n",
    "    y_pred = (np.array(train_scores) >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(train_targets, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch+1}')\n",
    "    plt.show()\n",
    "\n",
    "# Save the model\n",
    "model_path = 'cnn_model.pth'\n",
    "torch.save(cnn.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('cnn_model_architecture.pkl', 'wb') as f:\n",
    "    pickle.dump(cnn, f)\n",
    "print('Model architecture saved to cnn_model_architecture.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a27b7e-870e-41df-a89d-ded044585488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# for PBM\n",
    "def parse_fasta(file_path):\n",
    "    probe_ids = []\n",
    "    sequences = []\n",
    "\n",
    "    # Parse the FASTA file using SeqIO\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        probe_id = record.id  # Extract the probe ID\n",
    "        sequence = str(record.seq)  # Get the full sequence\n",
    "        sequence_of_interest = sequence[-35:]  # Extract the last 35 nucleotides\n",
    "        probe_ids.append(probe_id)\n",
    "        sequences.append(sequence_of_interest)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'probe_id': probe_ids,\n",
    "        'sequence': sequences\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test data\n",
    "file_path = \"1_test_PBM_participants.fasta\"  \n",
    "parsed_data = parse_fasta(file_path)\n",
    "\n",
    "Save the DataFrame to a CSV file for future use\n",
    "parsed_data.to_csv(\"parsed_sequences.csv\", index=False)\n",
    "\n",
    "\n",
    "Load the parsed data from the CSV file\n",
    "parsed_data = pd.read_csv(\"parsed_sequences.csv\")\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b33d9-1f06-49f1-898d-c0b832fe61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X_test_dna = parsed_data['sequence']\n",
    "probe_ids = parsed_data['probe_id']  \n",
    "\n",
    "# Encode the test sequences\n",
    "X_test_encoded = [torch.tensor(integer_encode_variable_kmers(seq, k_sizes), dtype=torch.long) for seq in X_test_dna]\n",
    "\n",
    "# Create a DataLoader for the test data\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.stack(X_test_encoded))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Make predictions\n",
    "cnn.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data[0].to(device) \n",
    "        outputs = cnn(data)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy().squeeze()  # Convert logits to probabilities\n",
    "        predictions.extend(probs)\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'probe_id': probe_ids,\n",
    "    'score': predictions\n",
    "})\n",
    "\n",
    "# Ensure the predictions are within [0,1] and have up to 5 decimal places as needed according to IBIS requirements\n",
    "predictions_df['score'] = predictions_df['score'].clip(0, 1).round(5)\n",
    "\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c362aad-ab44-4615-9be7-8f18eef27b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f886f4-90a9-46da-a5d4-f1f68e25e315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e68827-435c-40ea-bc41-72b57a0c6703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c859aa-64e1-4649-88db-c4f7efd3fcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d787c2b-3a29-4de7-9ebf-1f0a4da28330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
